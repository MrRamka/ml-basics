{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d21a8d9-726c-436d-a737-9be441bb65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "from utils import timing_decorator, get_images_dir_path\n",
    "import pandas\n",
    "from autoencoder import Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9842d83a-6fbd-410a-af73-a11de700fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = 'autoencoder_results'\n",
    "output_file_name = 'encoded_vectors.csv'\n",
    "generated_images_folder = 'generated_images'\n",
    "encoded_vectors = os.path.join(result_folder, output_file_name)\n",
    "results_rnn_file = os.path.join(result_folder, 'training_rnn_results.csv')\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859a8662-7ca1-4f10-b4ab-fc1ca535bb3e",
   "metadata": {},
   "source": [
    "### Загрузка векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f462de-1483-4d46-bb46-ba29254ea8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>encode_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_27358.jpg</td>\n",
       "      <td>8.11378,1.1557901,-2.1424499,-9.442376,6.71844...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_07088.jpg</td>\n",
       "      <td>-2.5879674,-4.5867686,3.0055764,0.22425225,-9....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_30624.jpg</td>\n",
       "      <td>4.237051,6.8330345,-5.237319,13.798186,1.47321...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_11832.jpg</td>\n",
       "      <td>-7.6320715,0.54696316,-8.56017,-0.63509077,3.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_20437.jpg</td>\n",
       "      <td>2.425571,12.118851,-22.992817,-6.7715807,-4.24...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         file_name                                      encode_vector\n",
       "0  image_27358.jpg  8.11378,1.1557901,-2.1424499,-9.442376,6.71844...\n",
       "1  image_07088.jpg  -2.5879674,-4.5867686,3.0055764,0.22425225,-9....\n",
       "2  image_30624.jpg  4.237051,6.8330345,-5.237319,13.798186,1.47321...\n",
       "3  image_11832.jpg  -7.6320715,0.54696316,-8.56017,-0.63509077,3.8...\n",
       "4  image_20437.jpg  2.425571,12.118851,-22.992817,-6.7715807,-4.24..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.read_csv(encoded_vectors, dtype={'file_name':str, 'encode_vector':str})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdaa7524-45dc-4f38-bb90-880b0032cf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47933 entries, 0 to 47932\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   file_name      47933 non-null  object\n",
      " 1   encode_vector  47933 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 749.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a68b0b5-b574-4c7e-9f8c-cb8092daf144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeArray(text):\n",
    "    return np.fromstring(text,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58492676-3526-4858-ae8b-d686ff231c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['encode_vector'] = df['encode_vector'].apply(makeArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "878f22e1-9357-4fe1-ae86-eab778e7d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7063875d-2c6b-4627-a44c-b53983df0367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>encode_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24157</th>\n",
       "      <td>image_00000.jpg</td>\n",
       "      <td>[-2.2850296, 2.439629, 1.491935, -9.362732, 6....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26829</th>\n",
       "      <td>image_00001.jpg</td>\n",
       "      <td>[-1.3955263, 3.1770823, -3.3733964, -5.185953,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29740</th>\n",
       "      <td>image_00002.jpg</td>\n",
       "      <td>[-3.070328, 5.6636457, -13.13622, -9.0368395, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27275</th>\n",
       "      <td>image_00003.jpg</td>\n",
       "      <td>[7.6021585, 9.363819, -23.031458, -10.117748, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35641</th>\n",
       "      <td>image_00004.jpg</td>\n",
       "      <td>[10.171621, -0.79664415, -18.933537, -2.322086...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33195</th>\n",
       "      <td>image_00005.jpg</td>\n",
       "      <td>[6.8616524, 13.113563, -12.714206, -3.946662, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30284</th>\n",
       "      <td>image_00006.jpg</td>\n",
       "      <td>[0.088547744, 2.4296923, -3.4688454, -0.509654...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32526</th>\n",
       "      <td>image_00007.jpg</td>\n",
       "      <td>[-5.4084187, -4.034243, 2.4636571, -4.7346554,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14710</th>\n",
       "      <td>image_00008.jpg</td>\n",
       "      <td>[-9.834157, -17.30501, -9.228772, -6.593273, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12367</th>\n",
       "      <td>image_00009.jpg</td>\n",
       "      <td>[-13.847381, -14.681672, -6.7845025, -8.794621...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35562</th>\n",
       "      <td>image_00010.jpg</td>\n",
       "      <td>[-3.500808, -6.130702, -11.83285, -1.673257, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33228</th>\n",
       "      <td>image_00011.jpg</td>\n",
       "      <td>[1.5185876, 4.010156, -9.922433, 2.2021163, 3....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             file_name                                      encode_vector\n",
       "24157  image_00000.jpg  [-2.2850296, 2.439629, 1.491935, -9.362732, 6....\n",
       "26829  image_00001.jpg  [-1.3955263, 3.1770823, -3.3733964, -5.185953,...\n",
       "29740  image_00002.jpg  [-3.070328, 5.6636457, -13.13622, -9.0368395, ...\n",
       "27275  image_00003.jpg  [7.6021585, 9.363819, -23.031458, -10.117748, ...\n",
       "35641  image_00004.jpg  [10.171621, -0.79664415, -18.933537, -2.322086...\n",
       "33195  image_00005.jpg  [6.8616524, 13.113563, -12.714206, -3.946662, ...\n",
       "30284  image_00006.jpg  [0.088547744, 2.4296923, -3.4688454, -0.509654...\n",
       "32526  image_00007.jpg  [-5.4084187, -4.034243, 2.4636571, -4.7346554,...\n",
       "14710  image_00008.jpg  [-9.834157, -17.30501, -9.228772, -6.593273, -...\n",
       "12367  image_00009.jpg  [-13.847381, -14.681672, -6.7845025, -8.794621...\n",
       "35562  image_00010.jpg  [-3.500808, -6.130702, -11.83285, -1.673257, -...\n",
       "33228  image_00011.jpg  [1.5185876, 4.010156, -9.922433, 2.2021163, 3...."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e22fc0-9e12-4bb5-842d-4c66fbf5939f",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c51c476-d571-418c-b5d9-cf44c33feeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e01933e5-de4d-4b67-9419-bf2bd6d0c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[i] - [from i to i + timesteps]\n",
    "# y[i] - i + timesteps element (target value )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0722bd0f-ddec-4187-b880-ae21377ea5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, timesteps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - timesteps):\n",
    "        X.append(data[i:i+timesteps])\n",
    "        y.append(data[i+timesteps])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1eeac5f0-5ac2-4f4a-b52f-38fc6f26e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.stack(df['encode_vector'].values)\n",
    "timesteps = 10\n",
    "X, y = create_dataset(data, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d1a2c6ce-c645-474d-9b16-7b811b16687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TimeSeriesDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=47923, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ffaee96c-5f37-4b30-9367-6ec49bbb9bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([47923, 10, 128])\n",
      "Feature [0] batch shape: torch.Size([10, 128])\n",
      "Outputs [0] batch shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "features, outputs = next(iter(dataloader))\n",
    "print(f\"Feature batch shape: {features.size()}\")\n",
    "print(f\"Feature [0] batch shape: {features[0].shape}\")\n",
    "print(f\"Outputs [0] batch shape: {outputs[0].shape}\")\n",
    "\n",
    "# print(features[0])\n",
    "# print(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9c6dd-ee21-4884-8c70-8d9572ccd6be",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8b199387-5815-44e5-93ae-cf8629a1039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 128\n",
    "timesteps = 10   # длина временного ряда, используемого для предсказания\n",
    "num_units = 128  # количество блоков LSTM в первом слое\n",
    "epochs = 100     \n",
    "learning_rate = 0.001\n",
    "num_layers = 3\n",
    "output_size = 128\n",
    "hidden_size = 128 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "46af387f-d60a-483f-984f-36b8b1659bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_units, num_layers):\n",
    "        super(SimpleLSTMModel, self).__init__()\n",
    "        # self.lstm = nn.LSTM(input_dim, num_units, num_layers, batch_first=True, bidirectional=False)\n",
    "        # self.fc = nn.Linear(num_units, input_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, num_units, num_layers, batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(num_units)\n",
    "        self.fc = nn.Linear(num_units, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        # x = self.fc(x[:, -1, :])\n",
    "\n",
    "        x = self.layer_norm(x[:, -1, :])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5069aa7e-c697-4414-a07c-1172550c72db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, num_units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Linear(num_units, 1, bias=False)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        attention_weights = torch.softmax(self.attention(lstm_output), dim=1)\n",
    "        context_vector = torch.sum(attention_weights * lstm_output, dim=1)\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "class ImprovedLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_units, num_layers):\n",
    "        super(ImprovedLSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, num_units, num_layers, batch_first=True)\n",
    "        self.attention = Attention(num_units)\n",
    "        self.layer_norm = nn.LayerNorm(num_units)\n",
    "        self.fc = nn.Linear(num_units, input_dim)\n",
    "        self.residual = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        context_vector = self.attention(lstm_out)\n",
    "        norm_out = self.layer_norm(context_vector)\n",
    "        output = self.fc(norm_out)\n",
    "        return output + self.residual(x[:, -1, :]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bbb54486-d7bd-4218-adef-93e668279dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing_decorator\n",
    "def train(num_epochs, dataloader, input_dim, num_units, num_layers):\n",
    "    # model = SimpleLSTMModel(input_dim, num_units, num_layers)\n",
    "    model = ImprovedLSTMModel(input_dim, num_units, num_layers)\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    model.to(device)\n",
    "\n",
    "    with open(results_rnn_file, 'w') as f:\n",
    "        f.write('Epoch,Loss\\n')\n",
    "\n",
    "    print(f'Обучение начато на {num_epochs} эпох')\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        for inputs, targets in dataloader:            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = loss_function(outputs.cpu(), targets)\n",
    "            loss.backward()\n",
    "\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'-> Эпоха: {epoch} Средний loss: {train_loss:.4f}')\n",
    "            with open(results_rnn_file, 'a') as f:\n",
    "                f.write(f'{epoch+1},{train_loss:.4f}\\n')\n",
    "\n",
    "        \n",
    "        train_loss /= len(dataloader)\n",
    "        scheduler.step(train_loss)\n",
    "    \n",
    "    torch.save(model.state_dict(), os.path.join(result_folder, f'rnn_model.pth'))\n",
    "    print('Окончание обучения')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3730d9f8-3373-4884-9851-5768838ee410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение начато на 150 эпох\n",
      "-> Эпоха: 0 Средний loss: 88.7387\n",
      "-> Эпоха: 5 Средний loss: 74.2161\n",
      "-> Эпоха: 10 Средний loss: 65.1022\n",
      "-> Эпоха: 15 Средний loss: 58.3757\n",
      "-> Эпоха: 20 Средний loss: 53.0212\n",
      "-> Эпоха: 25 Средний loss: 48.5835\n",
      "-> Эпоха: 30 Средний loss: 44.8102\n",
      "-> Эпоха: 35 Средний loss: 41.5577\n",
      "-> Эпоха: 40 Средний loss: 38.7382\n",
      "-> Эпоха: 45 Средний loss: 36.3139\n",
      "-> Эпоха: 50 Средний loss: 34.2740\n",
      "-> Эпоха: 55 Средний loss: 32.5308\n",
      "-> Эпоха: 60 Средний loss: 31.0200\n",
      "-> Эпоха: 65 Средний loss: 29.7286\n",
      "-> Эпоха: 70 Средний loss: 28.6052\n",
      "-> Эпоха: 75 Средний loss: 27.6344\n",
      "-> Эпоха: 80 Средний loss: 26.7965\n",
      "-> Эпоха: 85 Средний loss: 26.1063\n",
      "-> Эпоха: 90 Средний loss: 25.4912\n",
      "-> Эпоха: 95 Средний loss: 24.9499\n",
      "-> Эпоха: 100 Средний loss: 24.4646\n",
      "-> Эпоха: 105 Средний loss: 24.0343\n",
      "-> Эпоха: 110 Средний loss: 23.6385\n",
      "-> Эпоха: 115 Средний loss: 23.2635\n",
      "-> Эпоха: 120 Средний loss: 22.9505\n",
      "-> Эпоха: 125 Средний loss: 22.6347\n",
      "-> Эпоха: 130 Средний loss: 22.3597\n",
      "-> Эпоха: 135 Средний loss: 22.0909\n",
      "-> Эпоха: 140 Средний loss: 21.8441\n",
      "-> Эпоха: 145 Средний loss: 21.6064\n",
      "Окончание обучения\n",
      "Время выполнения train: 269.58 секунд\n"
     ]
    }
   ],
   "source": [
    "train(150, dataloader, input_dim, num_units, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9222e72b-60a3-4032-a945-1c7ee4dd77cb",
   "metadata": {},
   "source": [
    "#### Результаты LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a353bd34-33ac-4257-b69a-bcde562b257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lstm_model(model_path, input_dim, num_units, num_layers):\n",
    "    model = ImprovedLSTMModel(input_dim, num_units, num_layers)\n",
    "        \n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eebc643f-b1eb-4273-8331-95fc2e914e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_autoencoder_model(model_path):\n",
    "    model = Autoencoder()\n",
    "        \n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2d214e70-e63c-4ad8-80d6-0426f3304f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model_path = os.path.join(result_folder, 'rnn_model.pth')\n",
    "autoencoder_model_path = os.path.join(result_folder, 'autoencoder.pth')\n",
    "\n",
    "rnn_model = load_lstm_model(rnn_model_path, input_dim, num_units, num_layers)\n",
    "autoencoder_model = load_autoencoder_model(autoencoder_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05336881-112c-4a0b-a73d-52b328bed8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing_decorator\n",
    "def generate_vectors(model, initial_input, num_predictions):\n",
    "    model.to(device)\n",
    "    generated = []\n",
    "    input_seq = initial_input\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_predictions):\n",
    "            input_seq_tensor = torch.tensor(input_seq, dtype=torch.float32).unsqueeze(0)\n",
    "            input_seq_tensor = input_seq_tensor.to(device)\n",
    "\n",
    "            next_vector = model(input_seq_tensor)\n",
    "            next_vector = next_vector.squeeze(0).cpu().numpy()\n",
    "\n",
    "            # save vector \n",
    "            generated.append(next_vector)\n",
    "            # generate add to input new vector\n",
    "            # input_seq = input_seq[:10]\n",
    "            input_seq.append(next_vector)\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fd758c3f-0316-43dc-b232-03b1324463ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing_decorator\n",
    "def generate(test_input_vector, images_count=10):\n",
    "    new_vectors = generate_vectors(rnn_model, test_input_vector, images_count)\n",
    "    os.makedirs(generated_images_folder, exist_ok=True)\n",
    "    images = []\n",
    "    for i in range(len(new_vectors)):\n",
    "        vector = new_vectors[i]\n",
    "        tensor = torch.from_numpy(np.array([vector]))\n",
    "        tensor = tensor.to(device)\n",
    "        \n",
    "        outputs = autoencoder_model.decoder(tensor)\n",
    "        outputs = outputs[0].cpu().detach().squeeze(0)\n",
    "        images.append(outputs.numpy())\n",
    "        \n",
    "        file_path = os.path.join(generated_images_folder, f'generated_image_{i+1}.jpg')\n",
    "        \n",
    "        plt.imshow(outputs, cmap=\"gray\")\n",
    "        plt.axis('off')\n",
    "        plt.savefig(file_path)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "28d7b4f8-40bb-4691-ad3b-360538438640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создание изображений\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j5/fl61_2bn0kn0kbl76dhw03j80000gn/T/ipykernel_45157/582491005.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:277.)\n",
      "  input_seq_tensor = torch.tensor(input_seq, dtype=torch.float32).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время выполнения generate_vectors: 8.79 секунд\n",
      "Время выполнения generate: 156.93 секунд\n",
      "Готово\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_input_vector = [df.iloc[100]['encode_vector']]\n",
    "images_count=250\n",
    "\n",
    "print(\"Создание изображений\")\n",
    "images = generate(test_input_vector, images_count)\n",
    "print(\"Готово\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cebfb25-da75-4245-841d-7bda873d9de3",
   "metadata": {},
   "source": [
    "### Создание видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b201fda1-814f-49c0-bafd-16f963b40b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing_decorator\n",
    "def create_video_from_images(image_folder, output_path, fps=30, size=None):\n",
    "    \n",
    "    images = [img for img in os.listdir(image_folder) if img.endswith(\".jpg\")]\n",
    "    frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G') \n",
    "    video = cv2.VideoWriter(output_path, fourcc, 15, (width,height))\n",
    "    \n",
    "    for image in images:\n",
    "        video.write(cv2.imread(os.path.join(image_folder, image)))\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d4c064b2-7098-40a8-91f2-480e63dfb8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video/generated_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время выполнения create_video_from_images: 0.69 секунд\n"
     ]
    }
   ],
   "source": [
    "output_path = 'generated_video.mp4'\n",
    "video_folder = 'video'\n",
    "output_path = os.path.join(video_folder, output_path)\n",
    "print(output_path)\n",
    "create_video_from_images(generated_images_folder, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb46fc0-492d-4836-a87e-cb27b400a478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb07d4bc-3310-482b-baa3-bad6dee4ea58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
